{
    "cells": [
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "#  IBM Data Science Capstone: Car Accident Severity Report"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Introduction | Business Undertanding"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "As an effort to reduce the frequency of vehicular accidents in a city, an algorithim must be developed to predict the severity and the chances of a road accident given the current weather conditions, road and visibility. When conditions are bad, this model will alert drivers to remind them to be more careful, or to take a different route to their destination.\n\nIn most of the cases, not paying enough attention during driving, drug abuse or overspeeding are the main causes of serious accidents which would have been otherwise prevented by enacting harsher regulations. Several uncontrollable factors like the weather, visibility, or road conditions, etc are also contributing factors to a number of road accidents. These can be prevented by revealing hidden patterns in the data and warning to the local government, and notify the police and drivers traveling on those roads about the same. If these patterns are discovered early on, local government can know when to send alerts to the public and the respective authorities to drive more carefully or even avoid those roads entirely.\n\nThe target audience of the project is local Seattle government, police, rescue groups, and last but not least, car insurance institutes. The model and its results are going to provide some insights for the target audience to make important decisions for reducing the number of road accidents occuring in the city."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Data Understanding"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The data used in this project is taken from collision and accident reports in Seattle from the years 2004 to present. This data was collected by the Seattle Police Department(SPD) and the Traffic Records department of Seattle. The data has 37 independent variables and 194,673 records.\n\nWe will be using this data here to identify the key variables that may cause car accidents. For example, the \u201cWEATHER\u201d column may be used to show the types and number of accidents that occur for different weather conditions at the time of the accient. Furthermore, the \u201cINTKEY\u201d column can be grouped and the sum of the car accidents that happned at that paticular intersection can be known for all the intersections. This list can be sorted in descending order of the total sums to identify the most dangerous intersections, this data can be used by the respective authorities to provide better facilities and monitoring at that intersection. Finally, a supervised learning model will be used to come up with a formula that can predict the severity of an accident based on the inputs.\n\nOur predictor or target variable will be 'SEVERITYCODE' because it is used measure the severity of an accident from 0 to 5 within the dataset. Attributes used to weigh the severity of an accident are 'WEATHER', 'ROADCOND' and 'LIGHTCOND'. \u201cSEVERITYCODE\u201d contains numbers that correspond to different levels of severity caused by an accident from 0 to 4 which are as follows -\n\n0. Little to no Probability (Clear Conditions)\n1. Very Low Probability \u2014 Chance or Property Damage\n2. Low Probability \u2014 Chance of Injury\n3. Mild Probability \u2014 Chance of Serious Injury\n4. High Probability \u2014 Chance of Fatality"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Extracting Data and Pre-Processing"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "In it's original form, this data is not fit for analysis. For one, there are many columns that we will not use for this model. Also, most of the features are of type object, when they should be numerical type.\n\nWe must use label encoding to covert the features to our desired data type."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import pandas as pd\nimport numpy as np\n\nfile_name='https://s3.us.cloud-object-storage.appdomain.cloud/cf-courses-data/CognitiveClass/DP0701EN/version-2/Data-Collisions.csv'\ndf=pd.read_csv(file_name)\n\n# Drop all columns with no predictive value for the context of this project\ncolData = df.drop(columns = ['OBJECTID', 'SEVERITYCODE.1', 'REPORTNO', 'INCKEY', 'COLDETKEY', \n              'X', 'Y', 'STATUS','ADDRTYPE',\n              'INTKEY', 'LOCATION', 'EXCEPTRSNCODE',\n              'EXCEPTRSNDESC', 'SEVERITYDESC', 'INCDATE',\n              'INCDTTM', 'JUNCTIONTYPE', 'SDOT_COLCODE',\n              'SDOT_COLDESC', 'PEDROWNOTGRNT', 'SDOTCOLNUM',\n              'ST_COLCODE', 'ST_COLDESC', 'SEGLANEKEY',\n              'CROSSWALKKEY', 'HITPARKEDCAR', 'PEDCOUNT', 'PEDCYLCOUNT',\n              'PERSONCOUNT', 'VEHCOUNT', 'COLLISIONTYPE',\n              'SPEEDING', 'UNDERINFL', 'INATTENTIONIND'])\n\n# Label Encoding\n# Convert column to category\ncolData[\"WEATHER\"] = colData[\"WEATHER\"].astype('category')\ncolData[\"ROADCOND\"] = colData[\"ROADCOND\"].astype('category')\ncolData[\"LIGHTCOND\"] = colData[\"LIGHTCOND\"].astype('category')\n\n# Assign variable to new column for analysis\ncolData[\"WEATHER_CAT\"] = colData[\"WEATHER\"].cat.codes\ncolData[\"ROADCOND_CAT\"] = colData[\"ROADCOND\"].cat.codes\ncolData[\"LIGHTCOND_CAT\"] = colData[\"LIGHTCOND\"].cat.codes\n\ncolData.head(5)",
            "execution_count": 1,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (33) have mixed types. Specify dtype option on import or set low_memory=False.\n  interactivity=interactivity, compiler=compiler, result=result)\n",
                    "name": "stderr"
                },
                {
                    "output_type": "execute_result",
                    "execution_count": 1,
                    "data": {
                        "text/plain": "   SEVERITYCODE   WEATHER ROADCOND                LIGHTCOND  WEATHER_CAT  \\\n0             2  Overcast      Wet                 Daylight            4   \n1             1   Raining      Wet  Dark - Street Lights On            6   \n2             1  Overcast      Dry                 Daylight            4   \n3             1     Clear      Dry                 Daylight            1   \n4             2   Raining      Wet                 Daylight            6   \n\n   ROADCOND_CAT  LIGHTCOND_CAT  \n0             8              5  \n1             8              2  \n2             0              5  \n3             0              5  \n4             8              5  ",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SEVERITYCODE</th>\n      <th>WEATHER</th>\n      <th>ROADCOND</th>\n      <th>LIGHTCOND</th>\n      <th>WEATHER_CAT</th>\n      <th>ROADCOND_CAT</th>\n      <th>LIGHTCOND_CAT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>Overcast</td>\n      <td>Wet</td>\n      <td>Daylight</td>\n      <td>4</td>\n      <td>8</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Raining</td>\n      <td>Wet</td>\n      <td>Dark - Street Lights On</td>\n      <td>6</td>\n      <td>8</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>Overcast</td>\n      <td>Dry</td>\n      <td>Daylight</td>\n      <td>4</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>Clear</td>\n      <td>Dry</td>\n      <td>Daylight</td>\n      <td>1</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>Raining</td>\n      <td>Wet</td>\n      <td>Daylight</td>\n      <td>6</td>\n      <td>8</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "With the new columns, we can now use this data in our analysis and ML models!\n\nNow let's check the data types of the new columns in our dataframe. Moving forward, we will only use the new columns for our analysis."
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "colData.dtypes",
            "execution_count": 3,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 3,
                    "data": {
                        "text/plain": "SEVERITYCODE        int64\nWEATHER          category\nROADCOND         category\nLIGHTCOND        category\nWEATHER_CAT          int8\nROADCOND_CAT         int8\nLIGHTCOND_CAT        int8\ndtype: object"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "colData.describe()",
            "execution_count": 4,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 4,
                    "data": {
                        "text/plain": "        SEVERITYCODE    WEATHER_CAT   ROADCOND_CAT  LIGHTCOND_CAT\ncount  194673.000000  194673.000000  194673.000000  194673.000000\nmean        1.298901       2.977254       2.507122       4.256420\nstd         0.457778       2.892011       3.648660       1.900722\nmin         1.000000      -1.000000      -1.000000      -1.000000\n25%         1.000000       1.000000       0.000000       2.000000\n50%         1.000000       1.000000       0.000000       5.000000\n75%         2.000000       6.000000       7.000000       5.000000\nmax         2.000000      10.000000       8.000000       8.000000",
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SEVERITYCODE</th>\n      <th>WEATHER_CAT</th>\n      <th>ROADCOND_CAT</th>\n      <th>LIGHTCOND_CAT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>194673.000000</td>\n      <td>194673.000000</td>\n      <td>194673.000000</td>\n      <td>194673.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>1.298901</td>\n      <td>2.977254</td>\n      <td>2.507122</td>\n      <td>4.256420</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.457778</td>\n      <td>2.892011</td>\n      <td>3.648660</td>\n      <td>1.900722</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n      <td>-1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>2.000000</td>\n      <td>6.000000</td>\n      <td>7.000000</td>\n      <td>5.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>2.000000</td>\n      <td>10.000000</td>\n      <td>8.000000</td>\n      <td>8.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Balancing Dataset"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Analyzing Value Counts"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "colData[\"SEVERITYCODE\"].value_counts()",
            "execution_count": 5,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 5,
                    "data": {
                        "text/plain": "1    136485\n2     58188\nName: SEVERITYCODE, dtype: int64"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Our target variable SEVERITYCODE is only 42% balanced. In fact, severitycode in class 1 is nearly three times the size of class 2.\n\nWe can fix this by downsampling the majority class."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.utils import resample\n\n# Seperate majority and minority classes\ncolData_majority = colData[colData.SEVERITYCODE==1]\ncolData_minority = colData[colData.SEVERITYCODE==2]\n\n#Downsample majority class\ncolData_majority_downsampled = resample(colData_majority,\n                                        replace=False,\n                                        n_samples=58188,\n                                        random_state=123)\n\n# Combine minority class with downsampled majority class\ncolData_balanced = pd.concat([colData_majority_downsampled, colData_minority])\n\n# Display new class counts\ncolData_balanced.SEVERITYCODE.value_counts()\n\n",
            "execution_count": 7,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 7,
                    "data": {
                        "text/plain": "2    58188\n1    58188\nName: SEVERITYCODE, dtype: int64"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Now its Balanced!"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "We are using only 3 independent variables for our purpose, these are as follows --\n- Weather type (Clear, raining, etc.)\n- Road Condition (Dry, wet, etc.)\n- Light Condition (Daylight, dark, etc.)\n\nLets find out the total number of accidents using these individial parameters only!"
        },
        {
            "metadata": {
                "scrolled": true
            },
            "cell_type": "code",
            "source": "colData[\"WEATHER\"].value_counts()",
            "execution_count": 8,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 8,
                    "data": {
                        "text/plain": "Clear                       111135\nRaining                      33145\nOvercast                     27714\nUnknown                      15091\nSnowing                        907\nOther                          832\nFog/Smog/Smoke                 569\nSleet/Hail/Freezing Rain       113\nBlowing Sand/Dirt               56\nSevere Crosswind                25\nPartly Cloudy                    5\nName: WEATHER, dtype: int64"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "This proves that most accients took place in clear/dry weather."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "colData[\"ROADCOND\"].value_counts()",
            "execution_count": 9,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 9,
                    "data": {
                        "text/plain": "Dry               124510\nWet                47474\nUnknown            15078\nIce                 1209\nSnow/Slush          1004\nOther                132\nStanding Water       115\nSand/Mud/Dirt         75\nOil                   64\nName: ROADCOND, dtype: int64"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Here, we can see that the most number of accidents took place in dry weather! Followed by wet weather conditions. This is surprising but true."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "colData[\"LIGHTCOND\"].value_counts()",
            "execution_count": 10,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 10,
                    "data": {
                        "text/plain": "Daylight                    116137\nDark - Street Lights On      48507\nUnknown                      13473\nDusk                          5902\nDawn                          2502\nDark - No Street Lights       1537\nDark - Street Lights Off      1199\nOther                          235\nDark - Unknown Lighting         11\nName: LIGHTCOND, dtype: int64"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "From this data, it is clear that most car accidents took place in Daylight."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Methodology"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Our data is now ready to be fed into machine learning models. Statistical testing was not performed because the data revolved around categorical variables, not numerical ones.\n\nKey variables such as pedestrian right of way, inattentive drivers, and whether the car was speeding had a majority of null values. Therefore, they were dropped and not part of the analysis. However, it is likely that these variables play a key factor in vehicle accidents.\n\nWe will use the following models:\n\n - K-Nearest Neighbor (KNN)\nKNN will help us predict the severity code of an outcome by finding the most similar to data point within k distance.\n\n - Decision Tree\nA decision tree model gives us a layout of all possible outcomes so we can fully analyze the concequences of a decision. It context, the decision tree observes all possible outcomes of different weather conditions.\n\n - Logistic Regression\nBecause our dataset only provides us with two severity code outcomes, our model will only predict one of those two classes. This makes our data binary, which is perfect to use with logistic regression."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Initialization"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "X = np.asarray(colData_balanced[['WEATHER_CAT', 'ROADCOND_CAT', 'LIGHTCOND_CAT']])\nX[0:5]",
            "execution_count": 11,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 11,
                    "data": {
                        "text/plain": "array([[ 6,  8,  2],\n       [ 1,  0,  5],\n       [10,  7,  8],\n       [ 1,  0,  5],\n       [ 1,  0,  5]], dtype=int8)"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "y = np.asarray(colData_balanced['SEVERITYCODE'])\ny [0:5]",
            "execution_count": 12,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 12,
                    "data": {
                        "text/plain": "array([1, 1, 1, 1, 1])"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Normalize the dataset"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn import preprocessing\nX = preprocessing.StandardScaler().fit(X).transform(X)\nX[0:5]",
            "execution_count": 13,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int8 was converted to float64 by StandardScaler.\n  warnings.warn(msg, DataConversionWarning)\n/opt/conda/envs/Python36/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int8 was converted to float64 by StandardScaler.\n  warnings.warn(msg, DataConversionWarning)\n",
                    "name": "stderr"
                },
                {
                    "output_type": "execute_result",
                    "execution_count": 13,
                    "data": {
                        "text/plain": "array([[ 1.15236718,  1.52797946, -1.21648407],\n       [-0.67488   , -0.67084969,  0.42978835],\n       [ 2.61416492,  1.25312582,  2.07606076],\n       [-0.67488   , -0.67084969,  0.42978835],\n       [-0.67488   , -0.67084969,  0.42978835]])"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Train/Test Split\nWe will use 30% of our data for testing and 70% for training"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)\nprint ('Train set:', X_train.shape,  y_train.shape)\nprint ('Test set:', X_test.shape,  y_test.shape)",
            "execution_count": 14,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Train set: (81463, 3) (81463,)\nTest set: (34913, 3) (34913,)\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### K-Nearest Neighbors (KNN)"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Building the KNN Model\nfrom sklearn.neighbors import KNeighborsClassifier\n\nk = 25",
            "execution_count": 17,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "#Train Model & Predict  \nneigh = KNeighborsClassifier(n_neighbors = k).fit(X_train,y_train)\nneigh\n\nKyhat = neigh.predict(X_test)\nKyhat[0:5]",
            "execution_count": 18,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 18,
                    "data": {
                        "text/plain": "array([2, 2, 1, 1, 2])"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Decision Tree"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Building the Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ncolDataTree = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 7)\ncolDataTree\ncolDataTree.fit(X_train,y_train)",
            "execution_count": 19,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 19,
                    "data": {
                        "text/plain": "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=7,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter='best')"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Train Model & Predict\nDTyhat = colDataTree.predict(X_test)\nprint (y_test [0:5])",
            "execution_count": 20,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "[2 2 1 1 1]\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "### Logistic Regression"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Building the LR Model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nLR = LogisticRegression(C=6, solver='liblinear').fit(X_train,y_train)\nLR",
            "execution_count": 21,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 21,
                    "data": {
                        "text/plain": "LogisticRegression(C=6, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='warn',\n          n_jobs=None, penalty='l2', random_state=None, solver='liblinear',\n          tol=0.0001, verbose=0, warm_start=False)"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Train Model & Predicr\nLRyhat = LR.predict(X_test)\nLRyhat",
            "execution_count": 22,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 22,
                    "data": {
                        "text/plain": "array([1, 2, 1, ..., 2, 2, 2])"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "yhat_prob = LR.predict_proba(X_test)\nyhat_prob",
            "execution_count": 23,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 23,
                    "data": {
                        "text/plain": "array([[0.57295252, 0.42704748],\n       [0.47065071, 0.52934929],\n       [0.67630201, 0.32369799],\n       ...,\n       [0.46929132, 0.53070868],\n       [0.47065071, 0.52934929],\n       [0.46929132, 0.53070868]])"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "##  Results & Evaluation"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from sklearn.metrics import jaccard_similarity_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import log_loss",
            "execution_count": 33,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### K-Nearest Neighbor"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Jaccard Similarity Score\njaccard_similarity_score(y_test, Kyhat)",
            "execution_count": 34,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 34,
                    "data": {
                        "text/plain": "0.564001947698565"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# F1-SCORE\nf1_score(y_test, Kyhat, average='macro')",
            "execution_count": 35,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 35,
                    "data": {
                        "text/plain": "0.5401775308974308"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Decision Tree"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Jaccard Similarity Score\njaccard_similarity_score(y_test, DTyhat)",
            "execution_count": 36,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 36,
                    "data": {
                        "text/plain": "0.5664365709048206"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# F1-SCORE\nf1_score(y_test, DTyhat, average='macro')",
            "execution_count": 37,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 37,
                    "data": {
                        "text/plain": "0.5450597937389444"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Logistic Regression"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Jaccard Similarity Score\njaccard_similarity_score(y_test, LRyhat)",
            "execution_count": 38,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 38,
                    "data": {
                        "text/plain": "0.5260218256809784"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# F1-SCORE\nf1_score(y_test, LRyhat, average='macro')",
            "execution_count": 39,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 39,
                    "data": {
                        "text/plain": "0.511602093963383"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# LOGLOSS\nyhat_prob = LR.predict_proba(X_test)\nlog_loss(y_test, yhat_prob)",
            "execution_count": 40,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 40,
                    "data": {
                        "text/plain": "0.6849535383198887"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "\n### K-Nearest Neighbor\n\n- Jaccard Similarity Score = 0.563973305072609\n- F1-SCORE                 = 0.540128347154051\n\nTherefore, Model is most accurate when k is 25\n\n### Decision Tree\n\n- Jaccard Similarity Score = 0.5664365709048206\n- F1-SCORE                 = 0.5450597937389444\n\nTherefore, Model is most accurate with a max depth of 7.\n\n### Logistic Regression\n\n- Jaccard Similarity Score = 0.5260218256809784\n- F1-SCORE                 = 0.511602093963383\n- LOGLOSS                  = 0.6849535383198887\n\nModel is most accurate when hyperparameter C is 6."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Discussion\n\n\n\nIn the beginning of this notebook, we had categorical data that was of type 'object'. This is not a data type that we could have fed through an algorithm, so label encoding was used to created new classes that were of type int8; a numerical data type.\n\nAfter solving that issue we were presented with another - imbalanced data. As mentioned earlier, class 1 was nearly three times larger than class 2. The solution to this was downsampling the majority class with sklearn's resample tool. We downsampled to match the minority class exactly with 58188 values each.\n\nOnce we analyzed and cleaned the data, it was then fed through three ML models; K-Nearest Neighbor, Decision Tree and Logistic Regression. Although the first two are ideal for this project, logistic regression made the most sense because of its binary nature.\n\nEvaluation metrics used to test the accuracy of our models were jaccard index, f-1 score and logloss for logistic regression. Choosing different k, max depth and hyperamater C values helped to improve our accuracy to be the best possible.\n"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Conclusion \n\nBased on the dataset and the model provided for this capstone from weather, road, and light conditions pointing to certain classes, we can conclude that particular conditions have some kind of impact on - if travelling or not travelling in that particular weather condition could result in property damage of either Class - 1 or Class - 2. I.E., the current weather barely succeeds to describe the probability or the severity of the accident. But if you just follow the numbers, most cases of car accidents seem to be in Dry weather conditions(124510 cases), followed by Wet weather conditions(47474 cases), but this could be because people prefer to travel when there is  dry weather outside and maybe because these two weather conditions are prevalent throughout a major time period around the year. "
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}